# ğŸ¬ Video-to-Text-Translate

> Un outil puissant pour extraire, transcrire et traduire automatiquement le contenu audio des vidÃ©os. Parfait pour l'analyse de contenu vidÃ©o et la gÃ©nÃ©ration de sous-titres multilingues.

## âœ¨ FonctionnalitÃ©s

- ğŸµ **Extraction Audio** : Conversion de vidÃ©os en audio WAV optimisÃ©
- ğŸ—£ï¸ **Transcription Intelligente** : Utilisation de Whisper pour une transcription prÃ©cise
- ğŸŒ **Traduction Automatique** : Support multilingue avec mBART
- ğŸ“ **Format JSON** : Export structurÃ© avec timestamps et mÃ©tadonnÃ©es
- âš¡ **Performance** : OptimisÃ© pour Apple Silicon (M1/M2)
- ğŸ”„ **Traitement ParallÃ¨le** : Support du traitement de plusieurs vidÃ©os
- ğŸ§¹ **Nettoyage Automatique** : Gestion des fichiers temporaires
- ğŸ¯ **Synchronisation PrÃ©cise** : Maintien de la synchronisation audio-vidÃ©o avec prÃ©servation des segments non vocaux
- ğŸµ **Gestion Intelligente** : Conservation automatique de la musique et des effets sonores

## ğŸ› ï¸ PrÃ©requis

- ğŸ’» macOS (testÃ© sur Apple Silicon M1/M2)
- ğŸ Miniforge/Conda
- ğŸ¥ FFmpeg
- ğŸ’¾ 4GB d'espace disque minimum
- ğŸ§  8GB de RAM recommandÃ©s

## ğŸ“¥ Installation

### 1. Installation de Miniforge et des dÃ©pendances

Le projet utilise Miniforge pour gÃ©rer les dÃ©pendances Python et les packages natifs. Un script d'installation automatique est fourni :

```bash
# Rendre le script exÃ©cutable
chmod +x scripts/install_miniforge.sh

# Lancer l'installation
./scripts/install_miniforge.sh
```

Ce script va :
- ğŸ“¦ Installer Miniforge
- ğŸ CrÃ©er un environnement Conda `video-to-text`
- ğŸ”§ Installer toutes les dÃ©pendances nÃ©cessaires
- âœ… VÃ©rifier l'installation

### 2. Configuration

Le fichier `config.yaml` permet de configurer :
- ğŸ¤– Le modÃ¨le Whisper Ã  utiliser
- ğŸŒ Les options de traduction
- ğŸ“‚ Les chemins de sortie

Exemple de configuration :
```yaml
whisper:
  model: "base"  # Options: tiny, base, small, medium, large
  language: "auto"
  translate: true
  target_language: "fr"

translation:
  model: "facebook/mbart-large-50-many-to-many-mmt"
  max_length: 512

output:
  directory: "./output"
  temp_directory: "./output/temp"
  format: "json"
```

## ğŸš€ Utilisation

### Traitement d'une vidÃ©o locale

```bash
# Activer l'environnement Conda
conda activate video-to-text

# Traiter une vidÃ©o
python src/process_local_video.py chemin/vers/video.mp4
```

Options disponibles :
- `--config` : Chemin vers le fichier de configuration (par dÃ©faut : config.yaml)
- `--output` : RÃ©pertoire de sortie (par dÃ©faut : ./output)

### Format de sortie

Le rÃ©sultat est sauvegardÃ© dans un fichier JSON contenant :
- ğŸ“ Le texte original
- ğŸŒ Le texte traduit (si la traduction est activÃ©e)
- â±ï¸ Les segments avec timestamps
- ğŸŒ La langue dÃ©tectÃ©e
- ğŸ“… La date de traitement
- ğŸµ Les segments non vocaux (musique, applaudissements, etc.)

Exemple de sortie JSON :
```json
{
  "original_text": "Hello, this is a test video.",
  "translated_text": "Bonjour, ceci est une vidÃ©o test.",
  "segments": [
    {
      "start": 0.0,
      "end": 12.0,
      "text": "[Music]",
      "type": "non_vocal"
    },
    {
      "start": 12.0,
      "end": 16.0,
      "text": "[Applause]",
      "type": "non_vocal"
    },
    {
      "start": 16.0,
      "end": 20.0,
      "text": "Hello, this is a test video.",
      "type": "vocal"
    }
  ],
  "language": "en",
  "processing_date": "2024-05-20T12:00:00"
}
```

## ğŸ“ Structure du projet

```
.
â”œâ”€â”€ ğŸ“„ config.yaml           # Configuration du projet
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ ğŸ“„ install_miniforge.sh  # Script d'installation
â”‚   â””â”€â”€ ğŸ“„ process_local.sh      # Script de traitement
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ ğŸ“‚ processors/      # Processeurs
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ audio_extractor.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ whisper_processor.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ translator.py
â”‚   â””â”€â”€ ğŸ“‚ utils/          # Utilitaires
â”‚       â””â”€â”€ ğŸ“„ config.py
â””â”€â”€ ğŸ“‚ output/             # RÃ©sultats
```

## ğŸ’» DÃ©veloppement

### Environnement de dÃ©veloppement

```bash
# Activer l'environnement
conda activate video-to-text

# VÃ©rifier l'installation
python -c "import sentencepiece; import whisper; print('OK')"
```

### Tests

Pour tester le traitement d'une vidÃ©o :
```bash
python src/process_local_video.py ./scripts/video.webm
```

### ModÃ¨les Whisper Disponibles

| ModÃ¨le | Taille | VRAM Requise | Vitesse Relative |
|--------|---------|--------------|------------------|
| tiny   | 39M     | ~1GB         | ~10x            |
| base   | 74M     | ~1GB         | ~7x             |
| small  | 244M    | ~2GB         | ~4x             |
| medium | 769M    | ~5GB         | ~2x             |
| large  | 1550M   | ~10GB        | 1x              |

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Voici comment contribuer :

1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ© (`git checkout -b feature/AmazingFeature`)
3. Commit vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ™ Remerciements

- [OpenAI Whisper](https://github.com/openai/whisper) pour le modÃ¨le de transcription
- [Hugging Face](https://huggingface.co/) pour les modÃ¨les de traduction
- [FFmpeg](https://ffmpeg.org/) pour le traitement audio

### Traitement Audio

Le systÃ¨me gÃ¨re intelligemment diffÃ©rents types de segments audio :

1. **Segments Vocaux** :
   - Transcription et traduction
   - Synchronisation prÃ©cise avec l'audio original
   - Remplacement par l'audio TTS traduit

2. **Segments Non Vocaux** :
   - DÃ©tection automatique (musique, applaudissements, etc.)
   - Conservation de l'audio original
   - Maintien de la synchronisation

3. **Synchronisation** :
   - PrÃ©servation des timestamps originaux
   - MÃ©lange intelligent de l'audio TTS et de l'audio original
   - Gestion des transitions entre segments 